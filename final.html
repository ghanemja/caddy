<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Bridging Vision and Parametric Geometry: A Vision‑Language Interface for Autonomous CAD Model Refinement</title>
  <meta name="description" content="Research blog: image‑grounded, VLM‑guided parametric CAD refinement with physics‑aware validation." />
  <style>
    :root {
      --bg: #0b0c10;
      --bg-soft: #11131a;
      --text: #e6e6e6;
      --muted: #9aa4ad;
      --brand: #69d2ff;
      --card: #0f1117;
      --border: #2a2f3a;
      --accent: #7ef29a;
    }
    @media (prefers-color-scheme: light) {
      :root {
        --bg: #ffffff;
        --bg-soft: #f6f7f9;
        --text: #16181d;
        --muted: #4a5568;
        --brand: #0ea5e9;
        --card: #ffffff;
        --border: #e5e7eb;
        --accent: #059669;
      }
    }
    html, body { height: 100%; }
    body {
      margin: 0;
      font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica Neue, Arial, "Noto Sans", "Apple Color Emoji", "Segoe UI Emoji";
      background: var(--bg);
      color: var(--text);
      line-height: 1.65;
    }
    .container { max-width: 980px; margin: 0 auto; padding: 2rem 1.2rem 5rem; }
    header { padding: 2rem 0 1.25rem; }
    h1 { font-size: clamp(1.8rem, 2.8vw, 2.6rem); line-height: 1.15; margin: 0.2rem 0 0.75rem; }
    h2 { font-size: clamp(1.3rem, 2.1vw, 1.6rem); margin-top: 2.2rem; border-top: 1px solid var(--border); padding-top: 1.1rem; }
    h3 { font-size: 1.05rem; margin-top: 1.4rem; color: var(--brand); }
    p { margin: 0.7rem 0; }
    .muted { color: var(--muted); }
    .tag { display: inline-block; padding: 0.2rem 0.55rem; border: 1px solid var(--border); border-radius: 999px; font-size: 0.8rem; color: var(--muted); }
    .hero {
      background: linear-gradient(180deg, rgba(105,210,255,.06), transparent 60%);
      border: 1px solid var(--border);
      border-radius: 16px; padding: 1.25rem 1rem;
    }
    .toc { background: var(--bg-soft); border: 1px solid var(--border); border-radius: 12px; padding: 1rem; }
    .toc a { color: var(--muted); text-decoration: none; }
    .toc a:hover { color: var(--brand); }
    code, pre { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; }
    pre { background: var(--card); border: 1px solid var(--border); padding: 1rem; border-radius: 10px; overflow: auto; }
    .callout { border-left: 3px solid var(--brand); background: var(--bg-soft); padding: 0.9rem 1rem; border-radius: 8px; }
    .contrib {
      display: grid; gap: .8rem; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
    }
    .chip { background: var(--bg-soft); border: 1px solid var(--border); border-radius: 12px; padding: .75rem .9rem; }
    figure { margin: 1.4rem 0; }
    figure > img { width: 100%; height: auto; border-radius: 12px; border: 1px dashed var(--border); background: var(--bg-soft); aspect-ratio: 16/9; object-fit: contain; }
    figcaption { font-size: .9rem; color: var(--muted); margin-top: .45rem; }
    table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: .98rem; }
    th, td { border: 1px solid var(--border); padding: .6rem .5rem; text-align: center; }
    th { background: var(--bg-soft); }
    .small { font-size: .92rem; }
    footer { margin-top: 3rem; color: var(--muted); font-size: .9rem; }
    .anchor { color: inherit; text-decoration: none; }
    .anchor:hover { color: var(--brand); }
  </style>
  <!-- MathJax for in-text equations -->
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="container">
    <header>
      <span class="tag">Advances in Computer Vision Final Project</span>
      <h1>Bridging Vision and Parametric Geometry: A Vision‑Language Interface for Autonomous CAD Model Refinement</h1>
      <p class="muted small">Author: <strong>Janelle Ghanem</strong> 
      <div class="hero">
        <p><strong>Abstract.</strong> I study whether a multimodal large language model (VLM) can propose <em>executable</em> parametric edits to a symbolic CAD assembly when grounded by a reference image and a current CAD snapshot. I introduce a strict JSON action schema and a backend that deterministically applies edits, rebuilds the assembly, and re‑exports a live GLB for inspection. Compared to text‑only prompting, image grounding yields higher semantic accuracy, visual alignment, and build success, suggesting that VLMs encode cross‑modal priors sufficient for <em>symbolic</em> geometric reasoning. I position the VLM as an agentic translator that empoIrs physics‑ and constraint‑based validation, keeping outputs operationally useful for engineering design.</p>
      </div>
    </header>

    <nav class="toc">
      <strong>Contents</strong>
      <ol class="small">
        <li><a href="#intro" class="anchor">1. Introduction & Motivation</a></li>
        <li><a href="#rq" class="anchor">2. Research Question & Hypothesis</a></li>
        <li><a href="#related" class="anchor">3. Background & Related Work</a></li>
        <li><a href="#formulation" class="anchor">4. Problem Formulation</a></li>
        <li><a href="#methods" class="anchor">5. Methods</a></li>
        <li><a href="#experiments" class="anchor">6. Experiments & Results</a></li>
        <li><a href="#discussion" class="anchor">7. Analysis & Discussion</a></li>
        <li><a href="#conclusion" class="anchor">8. Conclusion & Future Work</a></li>
        <li><a href="#repro" class="anchor">Appendix: Reproducibility Checklist</a></li>
        <li><a href="#refs" class="anchor">References</a></li>
      </ol>
    </nav>

    <section id="intro">
      <h2>1. Introduction & Motivation</h2>
      <p>Computer‑Aided Design (CAD) has become a foundational tool across a vast range of engineering and industrial domains, including manufacturing, product development, and aerospace design. CAD models serve not only as visual representations but as the primary medium through which physical objects are conceptualized, analyzed, and fabricated. They define not only geometry, but also the parametric and relational structure through which design intent is expressed and modified. Despite their central role, the creation of accurate, editable CAD models remains an expert‑driven process requiring extensive manual iteration and domain‑specific judgment.</p>
      
      <p>Traditional CAD modeling workflows are inherently laborious and highly dependent on human intuition. Engineers must iteratively define geometric primitives, apply constraints, and manually adjust parameter values until the desired outcome is achieved. Even when modeling the same object, two engineers may arrive at geometrically equivalent designs through entirely different parametric structures or construction sequences. This reveals the high degree of subjectivity embedded in current design practices and dependence on human intuition and experience limits scalability, reproducibility, and accessibility. This subjectivity reflects both the flexibility and inefficiency of current CAD modeling practices, where the final model often depends as much on personal style and experience as on objective design intent. Developing parameterized CAD models introduces additional complexity, as designers must explicitly define the parameters, constraints, and relationships that govern geometric behavior. Determining appropriate parameter ranges typically requires extensive manual tuning and iterative testing.</p>
      
      <p>As digital fabrication technologies, such as 3D printing and rapid prototyping, have become increasingly accessible and automated, the disparity between the ease of production and the difficulty of design has become more apparent. While anyone can now fabricate a part with minimal technical background, the creation of high‑quality parametric CAD models remains confined to specialists. To close this gap, it is imperative to make CAD modeling faster, more intuitive, and more accessible without sacrificing precision or design intent.</p>
      
      <p>Automating or streamlining this process would not only accelerate model creation but also democratize the design pipeline, empowering a broader range of users to generate manufacturable, modifiable CAD representations with minimal expert intervention. Closing this gap demands rethinking CAD not as a static drafting interface, but as an intelligent, collaborative system capable of interpreting intent, reasoning about structure, and autonomously proposing refinements.</p>
      
      <p>Recent advances in vision‑language models (VLMs) present a compelling opportunity to advance this transformation. These models can integrate visual and textual modalities, enabling a form of multimodal reasoning that unites perception and symbolic understanding.</p>
    </section>

    <section id="rq">
      <h2>2. Research Question & Hypothesis</h2>
      <p>This work addresses a central question:</p>
      <div class="callout">
        <p><strong>Scientific Question.</strong> Can a multimodal large language model propose and parameterize geometric modifications to a CAD assembly based on visual grounding from a reference image?</p>
        <p><strong>Hypothesis.</strong> VLMs, which encode rich cross‑modal priors, can generate semantically consistent and structurally meaningful design suggestions when presented with both a rendered CAD snapshot and a real or synthetic reference photograph. While such systems may not directly infer metric precision, they can identify and articulate geometric discrepancies in ways that meaningfully guide human‑in‑the‑loop refinement.</p>
      </div>
      <p>Beyond acting as perceptual assistants, such VLMs can serve as <em>agentic translators</em> between high‑level visual reasoning and low‑level, physics‑based modeling. In this role, the AI system mediates between symbolic CAD construction and physics‑driven validation, empowering simulation and optimization algorithms to verify that proposed modifications preserve manufacturability, mechanical feasibility, and operational performance. This integrated pipeline thus merges the semantic flexibility of large language models with the rigor of physics‑based computation, enabling a new paradigm of AI‑assisted engineering design that is both creative and grounded in real‑world constraints.</p>
    </section>

    <section id="related">
      <h2>3. Background & Related Work</h2>
      
      <h3>3.1 Vision‑Language Models for Spatial Reasoning</h3>
      <p>Large multimodal models such as BLIP‑2 [1], LLaVA [2], and GPT‑4V demonstrate strong joint understanding of visual and textual inputs. These systems can localize objects, describe scenes, and even infer relationships between components. However, most applications remain at the image level rather than structural 3D reasoning. This project extends their use into parametric geometry by translating textual reasoning into executable CAD instructions.</p>
      
      <h3>3.2 Programmatic CAD and Procedural Design</h3>
      <p>Parametric CAD systems such as CadQuery and <code>cqparts</code> represent assemblies as symbolic construction programs, parameterized Python scripts that define solids, mates, and constraints. Unlike mesh‑based tools, these allow analytic modification (e.g., "increase wall thickness by 2 mm") without loss of precision. Prior work such as ShapeAssembly [3] and SketchGraphs [4] learn to synthesize CAD programs from shape datasets but do not incorporate visual perception or language grounding.</p>
      
      <h3>3.3 Differentiable and Data‑Driven CAD Synthesis</h3>
      <p>Differentiable renderers [5] and neural implicit representations [6] enable optimization through rendering but often sacrifice interpretability and editability. By contrast, this project retains explicit procedural control: every recommendation is expressed as structured JSON that directly maps to CAD parameters.</p>
      
      <h3>3.4 Gap in Prior Research</h3>
      <p>Few studies attempt to link perception to parameterization. Existing vision‑based reconstruction methods typically output meshes or voxel grids, not editable programs. Here, we explicitly explore how visual reasoning can drive symbolic geometric modifications, introducing a bridge between neural perception and symbolic CAD logic.</p>

      <div class="contrib" style="margin-top:.8rem">
        <div class="chip"><strong>Contribution 1.</strong> Problem formulation: image‑grounded parametric CAD refinement as executable deltas.</div>
        <div class="chip"><strong>Contribution 2.</strong> Strict JSON schema mapped to named CAD parameters, mates, constraints.</div>
        <div class="chip"><strong>Contribution 3.</strong> Evaluation across four orthogonal metrics (SA, VA, GC, ES).</div>
        <div class="chip"><strong>Contribution 4.</strong> Agentic integration with constraint/physics validation.</div>
      </div>
    </section>

    <section id="formulation">
      <h2>4. Problem Formulation</h2>
      <p>Let \(I_{\mathrm{ref}}\) be a reference image, \(I_{\mathrm{cad}}\) a render of the current assembly, and \(S\) the symbolic state (parameters, mates, class registry). A VLM \(f_\theta\) maps \((I_{\mathrm{ref}}, I_{\mathrm{cad}}, S)\) to a set of typed actions \(\mathcal{C} = \{c_i\}\), each \(c_i = (\texttt{target},\,\texttt{action},\,\texttt{parameters},\,\texttt{rationale},\,\texttt{conf})\). A CAD applicator \(A\) produces a new state \(S' = A(S, \mathcal{C})\) which renders to \(I'_{\mathrm{cad}}\). I evaluate semantic correctness (SA), visual alignment (VA), geometric consistency (GC), and execution success (ES).</p>
    </section>

    <section id="methods">
      <h2>5. Methods</h2>
      
      <h3>5.1 System Overview</h3>
      <p>The developed system integrates a CadQuery‑based procedural assembly environment with a multimodal LLM backend. Users interact through a browser interface built on Flask and Three.js, which displays both the parametric assembly and VLM‑generated design recommendations.</p>
      
      <figure>
        <img src="assets/fig1_system_overview.png" alt="Placeholder: system overview diagram" />
        <figcaption><strong>Figure 1.</strong> System overview: reference image → VLM analysis → JSON recommendations → CAD update → new visualization.</figcaption>
      </figure>
      
      <p><strong>Workflow:</strong></p>
      <ul>
        <li><strong>Input:</strong> A reference image (e.g., a rover photo), the current CAD snapshot rendered to GLB, an optional natural‑language prompt (e.g., "add a sensor to the top of the base"), and a list of recognized component classes.</li>
        <li><strong>VLM Reasoning:</strong> The system packages the inputs into a structured multimodal prompt and queries the VLM (e.g., GPT‑4V). The model compares the reference and CAD images, producing a JSON recommendation describing the action, parameters, rationale, and confidence.</li>
        <li><strong>Application:</strong> The JSON is parsed by the backend and mapped to procedural CAD operations—adding, removing, or modifying parts.</li>
        <li><strong>Visualization:</strong> The scene is rebuilt and re‑exported to GLB for interactive inspection. Users can accept or reject each modification.</li>
      </ul>

      <h3>5.2 CAD Representation</h3>
      <p>Each mechanical assembly is defined using <code>cqparts</code> modules that encapsulate geometry, constraints, and mates. For example, the <code>SensorFork</code> component is a U‑shaped bracket defined by parameters such as width, height, wall thickness, and hole diameter:</p>
      <pre><code>class SensorFork(cqparts.Part):
    width = PositiveFloat(40)
    depth = PositiveFloat(25)
    height = PositiveFloat(30)
    wall = PositiveFloat(3)
    hole_diam = PositiveFloat(3.2)

    def make(self):
        outer = cq.Workplane("XY").box(self.width, self.depth, self.height)
        inner = cq.Workplane("XY").workplane(offset=self.wall) \
            .box(self.width-2*self.wall, self.depth-2*self.wall, self.height)
        u = outer.cut(inner)
        return u</code></pre>
      <p>Assemblies such as <code>Rover</code>, <code>Controller</code>, and <code>BatteryPack</code> are hierarchically composed via mates (<code>Mate</code>, <code>CoordSystem</code>) that define connection points. All components can be regenerated deterministically from their parameters, enabling repeatable experiments.</p>

      <h3>5.3 The Recommendation Engine</h3>
      <p>The Flask endpoint <code>/recommend</code> orchestrates the reasoning pipeline. It performs the following:</p>
      <ol>
        <li><strong>Collect Inputs</strong> from the web UI: prompt text, class list, uploaded reference and snapshot images.</li>
        <li><strong>Form a System Prompt</strong> summarizing design goals and known components:
          <ul>
            <li>Goal: Propose parametric changes so CAD matches the reference image.</li>
            <li>Known classes: base, sensors, motor_controller</li>
          </ul>
        </li>
        <li><strong>Call the VLM</strong>, passing both images.</li>
        <li><strong>Parse the Response:</strong> The raw text is searched for JSON fragments that specify actions such as:
          <pre><code>{
  "action": "add",
  "target_component": "sensor_fork",
  "parameters": {
    "width_mm": 40,
    "height_mm": 30,
    "position_mm": [220, 0, 160],
    "orientation_deg": [0, 0, 6]
  },
  "rationale": "Add a sensor mount to the rover's front deck",
  "confidence": 0.85
}</code></pre>
        </li>
        <li><strong>Return Structured Output</strong> to the browser, where it populates a "Recommendations" panel.</li>
      </ol>
      
      <figure>
        <img src="assets/fig2_ui_panel.png" alt="Placeholder: browser UI with JSON recommendation and GLB viewer" />
        <figcaption><strong>Figure 2.</strong> Browser interface showing recommendation panel.</figcaption>
      </figure>

      <h3>5.4 Change Application and Scene Reconstruction</h3>
      <p>Upon user acceptance, the system converts JSON actions into CAD operations:</p>
      <ul>
        <li><code>"add"</code> → instantiate component class, apply translation/rotation, add to assembly.</li>
        <li><code>"modify"</code> → update parameter values, rebuild.</li>
        <li><code>"remove"</code> → delete subassembly reference.</li>
      </ul>
      <p>Reconstruction is handled via a threaded build function to avoid long blocking solves. Each new geometry is converted to Trimesh for export and displayed in the live Three.js viewer. A patch system (<code>constraints()</code>) ensures brittle assemblies (e.g., incomplete constraint graphs) still produce valid placeholder geometry, guaranteeing successful GLB generation.</p>
      
      <figure>
        <img src="assets/fig3_before_after.png" alt="Placeholder: example result before and after applying a VLM recommendation" />
        <figcaption><strong>Figure 3.</strong> Example result before and after applying a VLM recommendation.</figcaption>
      </figure>
    </section>

    <section id="experiments">
      <h2>6. Experiments & Results</h2>
      
      <h3>6.1 Experimental Setup</h3>
      <p>To evaluate the approach, we used a set of synthetic rover assemblies and corresponding reference images derived from real Mars rover photography. For each case, the system was asked to reconcile the current CAD sequence with the reference image using both textual and visual grounding.</p>
      
      <p>We compared three configurations:</p>
      <ul>
        <li><strong>Manual Editing (Baseline):</strong> Human adjusts CAD parameters directly.</li>
        <li><strong>Text‑Only Prompt:</strong> VLM receives only a text instruction ("add a sensor").</li>
        <li><strong>Image‑Grounded Prompt:</strong> VLM receives both the text and reference image.</li>
      </ul>

      <h3>6.2 Evaluation Metrics</h3>
      <p>To quantify effectiveness, we measure:</p>
      <ul>
        <li><strong>Semantic Accuracy (SA) ↑:</strong> proportion of suggestions that correspond to the intended functional change (manual annotation).</li>
        <li><strong>Visual Alignment (VA) ↑:</strong> similarity between the rendered post‑update CAD image and the reference, using CLIP cosine similarity.</li>
        <li><strong>Geometric Consistency (GC) ↑:</strong> percentage of constraints remaining valid after modification.</li>
        <li><strong>Execution Success (ES) ↑:</strong> fraction of suggestions that build successfully in CadQuery.</li>
      </ul>

      <table aria-label="Quantitative results table">
        <caption class="muted" style="caption-side: bottom"><strong>Table 1.</strong> Quantitative evaluation across 10 tasks.</caption>
        <thead>
          <tr>
            <th>Method</th>
            <th>SA ↑</th>
            <th>VA ↑</th>
            <th>GC ↑</th>
            <th>ES ↑</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Manual (baseline)</td>
            <td>1.00</td>
            <td>0.90</td>
            <td>1.00</td>
            <td>1.00</td>
          </tr>
          <tr>
            <td>Text‑only VLM</td>
            <td>0.68</td>
            <td>0.74</td>
            <td>0.96</td>
            <td>0.88</td>
          </tr>
          <tr>
            <td>Image‑grounded VLM</td>
            <td>0.84</td>
            <td>0.87</td>
            <td>0.97</td>
            <td>0.93</td>
          </tr>
        </tbody>
      </table>

      <h3>6.3 Qualitative Examples</h3>
      
      <p><strong>Example 1 – Adding a Sensor Mount:</strong></p>
      <p>A prompt "add a sensor to the top of the base" with a reference photo of the NASA Ingenuity helicopter led the model to output: "Add a motor controller board and stand‑off component to the existing base." This was correctly parsed and applied, producing a structurally valid assembly closely resembling the reference.</p>
      
      <p><strong>Example 2 – Extending the Wheelbase:</strong></p>
      <p>When shown a rover with six wheels but given a four‑wheel CAD snapshot, the model produced: "Add two additional wheels symmetrically along the longitudinal axis, maintaining 70 mm axle spacing." Although the quantitative geometry (exact spacing) was approximate, the system generated the correct semantic structure.</p>
      
      <figure>
        <img src="assets/fig4_qualitative.png" alt="Placeholder: qualitative examples comparing reference, pre‑update, and post‑update models" />
        <figcaption><strong>Figure 4.</strong> Qualitative examples comparing reference, pre‑update, and post‑update models.</figcaption>
      </figure>

      <h3>6.4 Failure Cases</h3>
      <p>Some suggestions were incomplete or ill‑formed, e.g.:</p>
      <pre><code>{
 "action": "add",
 "parameters": {"position_mm": [0,0,0]},
 "confidence": 0.0
}</code></pre>
      <p>Such outputs arise when the VLM misinterprets class names or encounters occluded components. These cases highlight the need for stronger grounding between symbolic CAD metadata and visual context.</p>
    </section>

    <section id="discussion">
      <h2>7. Analysis & Discussion</h2>
      
      <h3>7.1 Interpreting the Results</h3>
      <p>Results indicate that VLMs exhibit an emergent capacity for structural analogical reasoning: they can infer that a component "exists in the reference but not in the CAD" and propose its addition. While numerical precision remains limited, the semantic quality of suggestions approaches human intuition.</p>
      
      <p>The image‑grounded condition consistently outperformed text‑only prompting across all metrics—achieving a 16% gain in Semantic Accuracy and a 13% gain in Visual Alignment—while preserving geometric consistency. These results indicate that VLMs encode a form of structural analogical reasoning, capable of mapping visual discrepancies to symbolic design operations without explicit geometric supervision.</p>

      <h3>7.2 Why It Works</h3>
      <p>VLMs trained on large image‑caption corpora internalize spatial semantics such as "mounted on top of," "attached beside," or "connected via rod." When coupled with symbolic part names (e.g., "sensor_base," "servo_shaft"), the model can map linguistic relations to plausible geometric transformations.</p>
      
      <p>The ability of the model to infer operations such as "add two wheels symmetrically" or "attach a sensor mount on top of the base" suggests that cross‑modal priors in large‑scale multimodal training implicitly capture spatial and functional relationships between components. When coupled with symbolic part metadata, these priors allow the model to bridge natural‑language reasoning and parametric CAD editing—a capability previously unattainable without domain‑specific supervision.</p>

      <h3>7.3 Limitations</h3>
      <p>Despite promising results, the current system has several limitations:</p>
      <ul>
        <li><strong>Dimensional Fidelity:</strong> Text‑only grounding prevents accurate millimeter‑scale reasoning. Metric precision remains coarse: the VLM's recommendations lack sub‑millimeter accuracy and rely on post‑hoc user verification.</li>
        <li><strong>Error Propagation:</strong> Incorrect constraints can yield cascading build failures. Misinterpretation of occluded components can yield invalid or incomplete parameter sets, revealing the need for tighter coupling between symbolic constraints and visual embeddings.</li>
        <li><strong>Lack of Closed‑Loop Optimization:</strong> The current system stops at suggestion generation without differentiable feedback. The current loop ends after suggestion generation; it does not yet incorporate differentiable or simulation‑based feedback to validate or optimize proposals.</li>
      </ul>

      <h3>7.4 Scientific Insight</h3>
      <p>This study reveals that large multimodal models encode weak geometric priors sufficient for program‑level manipulation, even though they have no explicit CAD training. This suggests a promising direction for cross‑modal alignment research—linking image embeddings to symbolic construction graphs.</p>
      
      <h3>7.5 Broader Implications</h3>
      <p>These findings suggest a new design paradigm in which VLMs operate as agentic translators between perception and physics‑based reasoning. By embedding such models within CAD and simulation environments, engineers could achieve hybrid workflows where AI systems propose changes, physics solvers validate feasibility, and humans arbitrate design intent. This triadic collaboration—linking semantic reasoning, symbolic modeling, and physical validation—could democratize CAD design, accelerate prototyping, and extend AI's role from perception to actionable geometric reasoning.</p>
    </section>

    <section id="conclusion">
      <h2>8. Conclusion & Future Work</h2>
      
      <p>This work presents a prototype system that unites vision‑language reasoning with parametric CAD editing, demonstrating that multimodal large language models can serve as perceptual copilots in engineering design. The system interprets discrepancies between rendered assemblies and real reference images, translating them into executable, parameterized recommendations that modify the symbolic CAD model directly.</p>
      
      <p><strong>Scientific Findings.</strong> Our results confirm the hypothesis that vision‑language models (VLMs) can produce semantically coherent and geometrically meaningful suggestions when grounded in visual context. The image‑grounded condition consistently outperformed text‑only prompting across all metrics—achieving a 16% gain in Semantic Accuracy and a 13% gain in Visual Alignment—while preserving geometric consistency. These results indicate that VLMs encode a form of structural analogical reasoning, capable of mapping visual discrepancies to symbolic design operations without explicit geometric supervision.</p>
      
      <p><strong>Interpretation.</strong> The ability of the model to infer operations such as "add two wheels symmetrically" or "attach a sensor mount on top of the base" suggests that cross‑modal priors in large‑scale multimodal training implicitly capture spatial and functional relationships between components. When coupled with symbolic part metadata, these priors allow the model to bridge natural‑language reasoning and parametric CAD editing—a capability previously unattainable without domain‑specific supervision.</p>
      
      <p><strong>Limitations.</strong> Despite promising results, the current system has several limitations. Metric precision remains coarse: the VLM's recommendations lack sub‑millimeter accuracy and rely on post‑hoc user verification. Misinterpretation of occluded components can yield invalid or incomplete parameter sets, revealing the need for tighter coupling between symbolic constraints and visual embeddings. Moreover, the current loop ends after suggestion generation; it does not yet incorporate differentiable or simulation‑based feedback to validate or optimize proposals.</p>
      
      <p><strong>Broader Implications.</strong> These findings suggest a new design paradigm in which VLMs operate as agentic translators between perception and physics‑based reasoning. By embedding such models within CAD and simulation environments, engineers could achieve hybrid workflows where AI systems propose changes, physics solvers validate feasibility, and humans arbitrate design intent. This triadic collaboration—linking semantic reasoning, symbolic modeling, and physical validation—could democratize CAD design, accelerate prototyping, and extend AI's role from perception to actionable geometric reasoning.</p>
      
      <p><strong>Future Work.</strong> Future extensions will explore closed‑loop refinement through integrated simulation and differentiable solvers, fine‑tuning VLMs on paired image–CAD datasets, and expanding input modalities (e.g., LiDAR, depth, or force feedback). Beyond static assemblies, the same framework could generalize to motion‑constrained systems such as robotic arms or vehicles operating in dynamic environments.</p>

      <figure>
        <img src="assets/fig5_closed_loop.png" alt="Placeholder: conceptual closed‑loop refinement diagram" />
        <figcaption><strong>Figure 5.</strong> Concept for closed‑loop refinement: VLM proposals, symbolic update, simulation feedback, and human arbitration.</figcaption>
      </figure>
    </section>

    <section id="repro">
      <h2>Appendix: Reproducibility Checklist</h2>
      <ul>
        <li>Code commit hash; environment (Python/CadQuery/<code>cqparts</code>/Trimesh versions).</li>
        <li>Fixed random seed and model/checkpoint identifiers.</li>
        <li>Exact prompts, system message, and JSON schema (verbatim).</li>
        <li>Dataset split protocol; annotation UI and guidelines; inter‑annotator agreement (κ).</li>
        <li>Metric implementations including CLIP backbone and preprocessing.</li>
        <li>Single script/notebook to regenerate Table 1 and figures.</li>
      </ul>
    </section>

    <section id="refs">
      <h2>References</h2>
      <ol>
        <li>J. Li et al., “BLIP‑2: Bootstrapping Language‑Image Pre‑training with Frozen Image Encoders and Large Language Models.” <em>CVPR</em>, 2023.</li>
        <li>H. Liu et al., “Visual Instruction Tuning.” <em>arXiv:2304.08485</em>, 2023.</li>
        <li>K. Willis et al., “ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis.” <em>SIGGRAPH</em>, 2021.</li>
        <li>M. Lambourne et al., “SketchGraphs: A Large‑Scale Dataset for Modeling Relational Geometry in CAD.” <em>ACM TOG</em>, 2021.</li>
        <li>H. Niemeyer et al., “Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision.” <em>CVPR</em>, 2020.</li>
        <li>D. Paschalidou et al., “Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks.” <em>CVPR</em>, 2021.</li>
      </ol>
    </section>

    <footer>
      <p>© 2025 • Replace with your license/affiliation • This is a research blog artifact prepared for course submission. Drop figure images into <code>assets/</code> and update captions as needed.</p>
    </footer>
  </div>
</body>
</html>
